\section*{Introduction}
\label{sec:intro}

The increasing computational power of modern calculators has greatly contributed to the spread of more complex machine learning methods. For instance, artificial neural networks can be implemented on many hidden layers. This branch of neural network models is usually referred as Deep Neural Networks. With respect to the case of a shallow neural network (just one hidden layer), the number of parameters to be learned is significantly higher. Although such models have proved to be effective and expressively powerful \cite{MLbook}, there is a certain freedom in the configuration of the model.

In this report we discuss the tuning of a simple Deep Neural Network (DNN) model. This is indeed an hyper-parameter optimization problem: we wish to select a model capable of achieving good results (classification error) whilst reducing the overall complexity of the network (layers, number of neurons), taking into account various optimizer and activation functions. Moreover, the training data also accounts for variations in the performance of the model. All these optimization aspects are crucial for the effective applications of DNNs in real-world data.